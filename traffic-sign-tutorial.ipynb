{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on tutorial: Traffic sign classifier with Tensorflow\n",
    "\n",
    "## Preparation steps\n",
    "* You can download this jupyter notebook: `git clone https://github.com/olesalscheider/traffic-sign-tutorial`\n",
    "* Please install Tensorflow\n",
    " * `pip3 install tensorflow` for the CPU variant\n",
    " * `pip3 install tensorflow-gpu` if you have a GPU with CUDA and CUDNN support\n",
    " * More details on https://www.tensorflow.org/install/\n",
    "* Download and extract the traffic sign dataset (GTSRB) for this tutorial:\n",
    " * http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip\n",
    " * Unzip the dataset\n",
    " * Convert images to png\n",
    " * ... or execute `data/download.sh`\n",
    "\n",
    "## Prepare the dataset\n",
    "\n",
    "Let's start by splitting the data into a train and a test dataset. We store the filenames in two CSV files and use approximately 80% of the data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = os.path.join('data', 'GTSRB', 'Final_Training') # Path to the extracted data\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'train'), 'w') as train_file, open(os.path.join(DATA_PATH, 'test'), 'w') as test_file:\n",
    "    # Iterate over all image files in the training data directory and store the paths in the CSV files\n",
    "    for dirpath, dirnames, files in os.walk(DATA_PATH):\n",
    "        is_train_example = {}\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                _, label = os.path.split(dirpath) # The last directory name encodes the class of the training example\n",
    "                label = int(label) # Convert it to an integer (this strips the leading zeros)\n",
    "\n",
    "                # There are multiple images of each sign. The number before the '_' gives the sign number.\n",
    "                # Make sure that different images of the same sign are only stored either in the training\n",
    "                # or the test set.\n",
    "                sign_no = int(file.split('_')[0])\n",
    "\n",
    "                # Generate the string that should be stored in the CSV file. It is the image path and the class label.\n",
    "                line = os.path.join(dirpath, file) + '\\t' + str(label) + '\\n'\n",
    "\n",
    "                # Store the line either in the training or test CSV file\n",
    "                if not sign_no in is_train_example.keys():\n",
    "                    is_train_example[sign_no] = np.random.randint(0, 10) > 1 # keep 80% of the data for training\n",
    "                if is_train_example[sign_no]:\n",
    "                    train_file.writelines(line)\n",
    "                else:\n",
    "                    test_file.writelines(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Define a function that takes a line from the CSV file and returns the decoded image and label\n",
    "    def read_data(line):\n",
    "        path, label = tf.decode_csv(line, [[''], [0]], field_delim='\\t') # Decode the line\n",
    "        file = tf.read_file(path) # Read the binary data from the image file\n",
    "        image = tf.image.decode_png(file, 3) # Decode the image\n",
    "\n",
    "        # Resize the image to 48x48 pixels\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "        image = tf.image.resize_bilinear(image, [48, 48])\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "        image.set_shape([48, 48, 3])\n",
    "        return image, label\n",
    "\n",
    "    # Create the training dataset\n",
    "    train_dataset = tf.data.TextLineDataset(os.path.join(DATA_PATH, 'train'))\n",
    "    train_dataset = train_dataset.shuffle(30000) # Shuffle the training dataset\n",
    "    train_dataset = train_dataset.map(read_data, 2) # Call the previously defined function for each entry\n",
    "    train_dataset = train_dataset.repeat(2) # Repeat the dataset 2 times\n",
    "    train_dataset = train_dataset.batch(32) # Create batches with 32 training examples\n",
    "\n",
    "    # Create the test dataset\n",
    "    test_dataset = tf.data.TextLineDataset(os.path.join(DATA_PATH, 'test'))\n",
    "    test_dataset = test_dataset.map(read_data, 2) # Call the previously defined function for each entry\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "\n",
    "    # Create a generic iterator\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "\n",
    "    # Create initializer operations for the iterator. These assign either the test of train dataset\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    test_init_op = iterator.make_initializer(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "First we define a class for one ResNet module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULARIZER_WEIGHT = 1e-5\n",
    "\n",
    "class ResnetModule(tf.keras.Model):\n",
    "    def __init__(self, name, num_output_channels):\n",
    "        super().__init__(name=name)\n",
    "        self.num_output_channels = num_output_channels\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(self.num_output_channels,\n",
    "            (3, 3),\n",
    "            padding='same',\n",
    "            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(REGULARIZER_WEIGHT),\n",
    "            name='conv1')\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(self.num_output_channels,\n",
    "            (3, 3),\n",
    "            padding='same',\n",
    "            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(REGULARIZER_WEIGHT),\n",
    "            name='conv2')\n",
    "\n",
    "        self.conv3 = None\n",
    "        if input_shapes[-1] != self.num_output_channels:\n",
    "            self.conv3 = tf.keras.layers.Conv2D(self.num_output_channels,\n",
    "                (1, 1),\n",
    "                kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(REGULARIZER_WEIGHT),\n",
    "                name='conv3')\n",
    "        super().build(input_shapes)\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        x = self.bn1(x, training=True)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        if self.conv3:\n",
    "            residual = self.conv3(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn2(x, training=True)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a simple model that consists of some ResNet modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignModel(tf.keras.Model):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.first_conv = tf.keras.layers.Conv2D(32,\n",
    "            (7, 7),\n",
    "            strides=(2, 2),\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(REGULARIZER_WEIGHT),\n",
    "            name='first_conv')\n",
    "\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.maxpool1 = tf.keras.layers.MaxPooling2D(2, 2)\n",
    "        self.maxpool2 = tf.keras.layers.MaxPooling2D(2, 2)\n",
    "        self.maxpool3 = tf.keras.layers.MaxPooling2D(2, 2)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.module1 = ResnetModule('rm1', 64)\n",
    "        self.module2 = ResnetModule('rm2', 128)\n",
    "        self.module3 = ResnetModule('rm3', 256)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(43) # We have 43 classes\n",
    "\n",
    "    def call(self, image):\n",
    "        # Cast the image to float\n",
    "        x = tf.cast(image, tf.float32)\n",
    "        # normalize it to a range between -1 and 1\n",
    "        x = (x - tf.constant(128.0, tf.float32)) / tf.constant(128.0, tf.float32)\n",
    "\n",
    "        # Run the neural network layers on the image\n",
    "        x = self.first_conv(x)\n",
    "        x = self.bn1(x, training=True)\n",
    "        x = self.module1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.module2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.module3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model\n",
    "\n",
    "Now that we have a data reader and defined the model, we can train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the traffic light classifier. This might take a while...\n",
      "Step    0 - Accuracy: 0.0000, loss: 151.3727\n",
      "Step   20 - Accuracy: 0.2188, loss: 106.7918\n",
      "Step   40 - Accuracy: 0.2812, loss: 83.1193\n",
      "Step   60 - Accuracy: 0.2812, loss: 73.3664\n",
      "Step   80 - Accuracy: 0.7812, loss: 35.9986\n",
      "Step  100 - Accuracy: 0.5938, loss: 49.1144\n",
      "Step  120 - Accuracy: 0.5938, loss: 41.8743\n",
      "Step  140 - Accuracy: 0.7812, loss: 20.0612\n",
      "Step  160 - Accuracy: 0.8750, loss: 13.3664\n",
      "Step  180 - Accuracy: 0.9062, loss: 11.2692\n",
      "Step  200 - Accuracy: 0.8750, loss: 13.4957\n",
      "Step  220 - Accuracy: 0.7812, loss: 15.9311\n",
      "Step  240 - Accuracy: 1.0000, loss: 3.2403\n",
      "Step  260 - Accuracy: 0.9375, loss: 5.7716\n",
      "Step  280 - Accuracy: 1.0000, loss: 1.4531\n",
      "Step  300 - Accuracy: 1.0000, loss: 1.3075\n",
      "Step  320 - Accuracy: 0.9688, loss: 2.5226\n",
      "Step  340 - Accuracy: 0.9375, loss: 4.7758\n",
      "Step  360 - Accuracy: 1.0000, loss: 0.6766\n",
      "Step  380 - Accuracy: 1.0000, loss: 4.0926\n",
      "Step  400 - Accuracy: 0.9688, loss: 2.5101\n",
      "Step  420 - Accuracy: 1.0000, loss: 1.1969\n",
      "Step  440 - Accuracy: 0.9688, loss: 3.3419\n",
      "Step  460 - Accuracy: 0.9688, loss: 2.5331\n",
      "Step  480 - Accuracy: 0.9688, loss: 1.8521\n",
      "Step  500 - Accuracy: 1.0000, loss: 2.2920\n",
      "Step  520 - Accuracy: 1.0000, loss: 0.7944\n",
      "Step  540 - Accuracy: 0.9375, loss: 2.9636\n",
      "Step  560 - Accuracy: 0.9688, loss: 2.5089\n",
      "Step  580 - Accuracy: 1.0000, loss: 1.0087\n",
      "Step  600 - Accuracy: 1.0000, loss: 0.5375\n",
      "Step  620 - Accuracy: 1.0000, loss: 0.1870\n",
      "Step  640 - Accuracy: 1.0000, loss: 0.1264\n",
      "Step  660 - Accuracy: 1.0000, loss: 0.2129\n",
      "Step  680 - Accuracy: 1.0000, loss: 0.2001\n",
      "Step  700 - Accuracy: 1.0000, loss: 0.1229\n",
      "Step  720 - Accuracy: 1.0000, loss: 0.3277\n",
      "Step  740 - Accuracy: 1.0000, loss: 0.3333\n",
      "Step  760 - Accuracy: 1.0000, loss: 0.0441\n",
      "Step  780 - Accuracy: 1.0000, loss: 0.1292\n",
      "Step  800 - Accuracy: 1.0000, loss: 0.1111\n",
      "Step  820 - Accuracy: 1.0000, loss: 0.1302\n",
      "Step  840 - Accuracy: 1.0000, loss: 0.3735\n",
      "Step  860 - Accuracy: 0.9688, loss: 2.3493\n",
      "Step  880 - Accuracy: 1.0000, loss: 0.7482\n",
      "Step  900 - Accuracy: 1.0000, loss: 0.0380\n",
      "Step  920 - Accuracy: 1.0000, loss: 0.6786\n",
      "Step  940 - Accuracy: 1.0000, loss: 0.1423\n",
      "Step  960 - Accuracy: 1.0000, loss: 0.1850\n",
      "Step  980 - Accuracy: 1.0000, loss: 0.3483\n",
      "Step 1000 - Accuracy: 1.0000, loss: 0.0406\n",
      "Step 1020 - Accuracy: 1.0000, loss: 0.0438\n",
      "Step 1040 - Accuracy: 1.0000, loss: 0.0286\n",
      "Step 1060 - Accuracy: 1.0000, loss: 0.2659\n",
      "Step 1080 - Accuracy: 1.0000, loss: 0.2442\n",
      "Step 1100 - Accuracy: 1.0000, loss: 0.1366\n",
      "Step 1120 - Accuracy: 1.0000, loss: 0.0357\n",
      "Step 1140 - Accuracy: 1.0000, loss: 0.0521\n",
      "Step 1160 - Accuracy: 1.0000, loss: 0.0315\n",
      "Step 1180 - Accuracy: 1.0000, loss: 0.1122\n",
      "Step 1200 - Accuracy: 1.0000, loss: 0.4092\n",
      "Step 1220 - Accuracy: 1.0000, loss: 0.0369\n",
      "Step 1240 - Accuracy: 1.0000, loss: 0.0740\n",
      "Step 1260 - Accuracy: 1.0000, loss: 0.0231\n",
      "Step 1280 - Accuracy: 1.0000, loss: 0.1916\n",
      "Step 1300 - Accuracy: 1.0000, loss: 0.0309\n",
      "Step 1320 - Accuracy: 1.0000, loss: 0.0301\n",
      "Step 1340 - Accuracy: 1.0000, loss: 0.0349\n",
      "Step 1360 - Accuracy: 1.0000, loss: 0.1805\n",
      "Step 1380 - Accuracy: 1.0000, loss: 0.0254\n",
      "Step 1400 - Accuracy: 1.0000, loss: 0.0328\n",
      "Step 1420 - Accuracy: 1.0000, loss: 0.0213\n",
      "Step 1440 - Accuracy: 1.0000, loss: 0.1554\n",
      "Step 1460 - Accuracy: 1.0000, loss: 0.0306\n",
      "Step 1480 - Accuracy: 1.0000, loss: 0.0325\n",
      "Step 1500 - Accuracy: 1.0000, loss: 0.0393\n",
      "Step 1520 - Accuracy: 1.0000, loss: 0.0456\n",
      "Step 1540 - Accuracy: 1.0000, loss: 0.0655\n",
      "Step 1560 - Accuracy: 1.0000, loss: 0.0539\n",
      "Step 1580 - Accuracy: 0.9688, loss: 1.4353\n",
      "Step 1600 - Accuracy: 1.0000, loss: 0.0395\n",
      "Step 1620 - Accuracy: 1.0000, loss: 0.0525\n",
      "Step 1640 - Accuracy: 1.0000, loss: 0.0196\n",
      "Step 1660 - Accuracy: 1.0000, loss: 0.0807\n",
      "Step 1680 - Accuracy: 1.0000, loss: 0.0420\n",
      "Step 1700 - Accuracy: 1.0000, loss: 0.0243\n",
      "Step 1720 - Accuracy: 1.0000, loss: 0.0975\n",
      "Step 1740 - Accuracy: 1.0000, loss: 0.0442\n",
      "Step 1760 - Accuracy: 1.0000, loss: 0.0211\n",
      "Step 1780 - Accuracy: 1.0000, loss: 0.0277\n",
      "Step 1800 - Accuracy: 1.0000, loss: 0.0322\n",
      "Step 1820 - Accuracy: 1.0000, loss: 0.0165\n",
      "Step 1840 - Accuracy: 1.0000, loss: 0.0338\n",
      "Step 1860 - Accuracy: 1.0000, loss: 0.0163\n",
      "Step 1880 - Accuracy: 1.0000, loss: 0.0219\n",
      "Step 1900 - Accuracy: 1.0000, loss: 0.0203\n",
      "Step 1920 - Accuracy: 1.0000, loss: 0.0733\n",
      "Step 1940 - Accuracy: 1.0000, loss: 0.0326\n",
      "Step 1960 - Accuracy: 1.0000, loss: 0.1672\n",
      "Evaluate the classifier. This might take a while...\n",
      "Mean accuracy on the test dataset: 0.9572\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    session = tf.Session()\n",
    "\n",
    "    # Instantiate the model we want to train\n",
    "    net = TrafficSignModel('net')\n",
    "    images, labels = iterator.get_next()\n",
    "    logits = net(images)\n",
    "\n",
    "    # Define the loss\n",
    "    loss_op = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "    loss_op += tf.reduce_sum(net.losses) # Add regularizer losses\n",
    "\n",
    "    # Define an OP to calculate the accuracy\n",
    "    correct_prediction = tf.equal(tf.cast(tf.argmax(logits, 1), tf.int32), labels)\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Define the learning rate and the optimizer.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(1e-3, global_step, 500, 0.5)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss_op, global_step=global_step)\n",
    "    train_op = tf.group(train_op, net.updates) # Add batch norm updates\n",
    "\n",
    "    # Initialize the model variables (randomly).\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print('Train the traffic light classifier. This might take a while...')\n",
    "    # Initialize the dataset iterator for training.\n",
    "    session.run(train_init_op)\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            _, accuracy, loss = session.run([train_op, accuracy_op, loss_op])\n",
    "            if i % 20 == 0:\n",
    "                print('Step %4i - Accuracy: %.4f, loss: %.4f' % (i, accuracy, loss))\n",
    "            i += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break # We finished!\n",
    "    \n",
    "    print('Evaluate the classifier. This might take a while...')\n",
    "    # Initialize the dataset iterator for training.\n",
    "    session.run(test_init_op)\n",
    "    i = 0\n",
    "    total_accuracy = 0.0\n",
    "    while True:\n",
    "        try:\n",
    "            accuracy = session.run(accuracy_op)\n",
    "            total_accuracy += accuracy\n",
    "            i += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break # We finished!\n",
    "    print('Mean accuracy on the test dataset: %.4f' % (total_accuracy / i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
